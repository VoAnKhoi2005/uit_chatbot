{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-13T16:45:06.799022Z",
     "start_time": "2025-10-13T16:45:06.791112Z"
    }
   },
   "source": [
    "import random\n",
    "import sqlite3\n",
    "\n",
    "def extract_from_db(cursor, id: int, include_parent=False):\n",
    "    cursor.execute(\"SELECT * FROM laws WHERE id = ?\", (id,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    results = [dict(zip(columns, row)) for row in rows]\n",
    "\n",
    "    if include_parent:\n",
    "        all_nodes = []\n",
    "        for r in results:\n",
    "            current = r\n",
    "            while current['parent_id'] is not None:\n",
    "                cursor.execute(\"SELECT * FROM laws WHERE id = ?\", (current['parent_id'],))\n",
    "                parent = cursor.fetchone()\n",
    "                if parent is None:\n",
    "                    break\n",
    "                parent_dict = dict(zip(columns, parent))\n",
    "                all_nodes.append(parent_dict)\n",
    "                current = parent_dict\n",
    "        results.extend(all_nodes)\n",
    "\n",
    "    return results[::-1]\n",
    "\n",
    "def extract_random_from_db(cursor, include_parent=True):\n",
    "    cursor.execute(\"SELECT * FROM laws WHERE title LIKE '%Điểm%'\")\n",
    "    rows = cursor.fetchall()\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    row = random.choice(rows)\n",
    "    result = dict(zip(columns, row))\n",
    "\n",
    "    results = [result]\n",
    "\n",
    "    if include_parent:\n",
    "        current = result\n",
    "        while current['parent_id'] is not None:\n",
    "            cursor.execute(\"SELECT * FROM laws WHERE id = ?\", (current['parent_id'],))\n",
    "            parent = cursor.fetchone()\n",
    "            if parent is None:\n",
    "                break\n",
    "            parent_dict = dict(zip(columns, parent))\n",
    "            results.append(parent_dict)\n",
    "            current = parent_dict\n",
    "\n",
    "    return results[::-1]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T15:47:27.241693Z",
     "start_time": "2025-10-15T15:47:26.373403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_gpt_response(user_prompt, system_prompt, api_key, model=\"gpt-4o\"):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content"
   ],
   "id": "cb0a0a46bc4ce861",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T16:23:58.541841Z",
     "start_time": "2025-10-13T16:23:58.538942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove newlines, tabs, extra spaces, punctuation and lowercase.\"\"\"\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[!?]+\", \"\", text)\n",
    "    return text.strip().lower()"
   ],
   "id": "19e966d4401e5998",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:46:17.332343Z",
     "start_time": "2025-10-11T06:46:17.327959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect(r\"E:\\Github\\LawAssistant\\triplet_extraction\\law.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Các bảng trong law.db:\", tables)"
   ],
   "id": "efe2018cc6c65ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các bảng trong law.db: [('laws',), ('law_refs',)]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:46:20.353548Z",
     "start_time": "2025-10-11T06:46:20.317272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = extract_random_from_db(cursor, True)\n",
    "law = \"\"\n",
    "title = \"\"\n",
    "for r in rows:\n",
    "    title += r['title'] + \" \"\n",
    "    if not (r['title'].strip().startswith(\"Chương\") or  r['title'].strip().startswith(\"Điều\")):\n",
    "        law += r['content'] + \"\\n\"\n",
    "\n",
    "so_hieu = r['so_hieu']\n",
    "id = r['id']\n",
    "print(id)\n",
    "print(so_hieu + \" \" + title)\n",
    "print(clean_text(law))"
   ],
   "id": "2a54da31325a9c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80a675aa13c6bad025f55461c749be4d0f554787e7f2254b6ad85561a3029755\n",
      "92/2015/QH13 Chương XXXIX Điều 482 Khoản 2 Điểm a \n",
      "những bản án, quyết định sau đây của tòa án cấp sơ thẩm được thi hành ngay mặc dù có thể bị kháng cáo, khiếu nại, kháng nghị, kiến nghị: bản án, quyết định về cấp dưỡng, trả công lao động, nhận người lao động trở lại làm việc, trả lương, trợ cấp thôi việc, trợ cấp mất sức lao động, trợ cấp mất việc làm, bảo hiểm xã hội, bảo hiểm thất nghiệp, bảo hiểm y tế hoặc bồi thường thiệt hại về tính mạng, sức khoẻ, tổn thất tinh thần của công dân; quyết định về tính hợp pháp của cuộc đình công;\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:46:23.360044Z",
     "start_time": "2025-10-11T06:46:23.353790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = extract_from_db(cursor, \"b7fee66c5d809a5fd7023b6810338639e4b19563187a93e21e17d4e21e92da77\", True)\n",
    "law = \"\"\n",
    "title = \"\"\n",
    "for r in rows:\n",
    "    title += r['title'] + \" \"\n",
    "    if not (r['title'].strip().startswith(\"Chương\") or r['title'].strip().startswith(\"Điều\")):\n",
    "        law += r['content'] + \"\\n\"\n",
    "\n",
    "so_hieu = r['so_hieu']\n",
    "id = r['id']\n",
    "print(id)\n",
    "print(so_hieu + \" \" + title)\n",
    "print(clean_text(law))"
   ],
   "id": "b8e287586659f72e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7fee66c5d809a5fd7023b6810338639e4b19563187a93e21e17d4e21e92da77\n",
      "36/2024/QH15 Chương IV Điều 57 Khoản 1 Điểm p \n",
      "giấy phép lái xe bao gồm các hạng sau đây: hạng de cấp cho người lái các loại xe ô tô quy định cho giấy phép lái xe hạng d kéo rơ moóc có khối lượng toàn bộ theo thiết kế trên 750 kg; xe ô tô chở khách nối toa.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T04:38:39.180735Z",
     "start_time": "2025-10-11T04:38:39.175739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt_rewrite = \"\"\"\n",
    "Bạn là trợ lý AI Tiếng Việt chuyên nghiệp và trung thực.\n",
    "Bạn là chuyên gia pháp luật Việt Nam, am hiểu các bộ luật, nghị định, và văn bản pháp luật.\n",
    "Bạn là chuyên gia ngôn ngữ Việt Nam, biết viết câu chuẩn cấu trúc, chính xác, trang trọng, và đúng ngôn ngữ pháp lý.\n",
    "Luôn trả lời chính xác, hữu ích, ngắn gọn và an toàn.\n",
    "Nếu thông tin không hợp lý hoặc thiếu, hãy yêu cầu thêm thông tin thay vì đoán mò.\n",
    "Không thay đổi ý nghĩa khi viết lại câu.\n",
    "Luôn dùng ngôn ngữ chính xác như trong văn bản pháp luật, tránh ngôn ngữ thông thường hay không trang trọng.\n",
    "\n",
    "Định nghĩa 'câu đơn': Một câu đơn là câu có một chủ ngữ (hoặc cụm chủ ngữ) và một vị ngữ (hoặc cụm vị ngữ), biểu đạt một ý trọn vẹn; câu có thể chứa thành tố phụ (tính từ, trạng từ, bổ ngữ) nhưng không được ghép bằng liên từ hoặc dấu câu như dấu \",\", \";\" để tạo hai hoặc nhiều mệnh đề độc lập.\n",
    "\n",
    "Quy tắc bắt buộc:\n",
    "1. Khi viết lại, **chỉ** trả về các câu đơn theo đúng định nghĩa trên; mỗi câu một dòng nếu có nhiều câu.\n",
    "2. **Được phép tái sử dụng** các thành phần câu (chủ ngữ, cụm danh từ, đại từ, cụm tính từ, v.v.) từ vế trước hoặc từ phần khác của câu gốc để hoàn chỉnh vế thiếu, **nhằm bảo toàn ý nghĩa** sau khi tách.\n",
    "3. Khi tái sử dụng, **ưu tiên giữ nguyên** từ ngữ gốc; chỉ thực hiện điều chỉnh nhỏ cần thiết để tạo câu đơn ngữ pháp đúng, **không** thêm thông tin, suy đoán hay nội dung mới.\n",
    "4. Tuyệt đối không kèm chú giải, giải thích, danh sách hay bất kỳ nội dung nào khác ngoài các câu viết lại.\n",
    "5. Nếu câu gốc mơ hồ hoặc thiếu thông tin đến mức không thể tạo câu đơn hoàn chỉnh mà vẫn giữ nguyên ý, hãy yêu cầu thêm thông tin ngắn gọn.\n",
    "Danh mục liên từ cần loại trừ khi viết câu đơn: và, hoặc, hoặc là, hay, hay là, nhưng, song, tuy nhiên, mà, còn, rồi.\n",
    "Giữ nguyên thứ tự trước sau của các từ sau khi viết lại câu.\n",
    "Sau khi viết lại câu không được thiếu từ danh từ nào trong câu gốc và phải độc lập không phụ thuộc vào câu trước đó.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt_rewrite = f\"\"\"\n",
    "Ngữ cảnh: Bộ luật số {so_hieu} trong luật Việt Nam\n",
    "Nhiệm vụ: Viết lại câu sau để hoàn chỉnh cấu trúc với đầy đủ chủ ngữ và vị ngữ, giữ nguyên ý nghĩa. Mỗi câu xuất ra phải là một câu đơn đầy đủ (một dòng một câu nếu có nhiều câu).\n",
    "Câu cần viết lại: \"{clean_text(law)}\"\n",
    "\"\"\"\n"
   ],
   "id": "ec368db79686e145",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T04:38:45.628543Z",
     "start_time": "2025-10-11T04:38:45.624291Z"
    }
   },
   "cell_type": "code",
   "source": "print(user_prompt_rewrite)",
   "id": "56312ee8787ed293",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ngữ cảnh: Bộ luật số 36/2024/QH15 trong luật Việt Nam\n",
      "Nhiệm vụ: Viết lại câu sau để hoàn chỉnh cấu trúc với đầy đủ chủ ngữ và vị ngữ, giữ nguyên ý nghĩa. Mỗi câu xuất ra phải là một câu đơn đầy đủ (một dòng một câu nếu có nhiều câu).\n",
      "Câu cần viết lại: \"giấy phép lái xe bao gồm các hạng sau đây: hạng de cấp cho người lái các loại xe ô tô quy định cho giấy phép lái xe hạng d kéo rơ moóc có khối lượng toàn bộ theo thiết kế trên 750 kg; xe ô tô chở khách nối toa.\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:33:47.431776Z",
     "start_time": "2025-10-11T06:33:47.415750Z"
    }
   },
   "cell_type": "code",
   "source": "rewrite_sentence = get_gpt_response(user_prompt_rewrite, system_prompt_rewrite, api_key, \"gpt-4.1-mini\")",
   "id": "2df1bca79ab853af",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_prompt_rewrite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m rewrite_sentence = get_gpt_response(\u001B[43muser_prompt_rewrite\u001B[49m, system_prompt_rewrite, api_key, \u001B[33m\"\u001B[39m\u001B[33mgpt-4.1-mini\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'user_prompt_rewrite' is not defined"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:46:33.976631Z",
     "start_time": "2025-10-11T06:46:33.974030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rewrite_sentence = \"\"\"\n",
    "Giấy phép lái xe bao gồm các hạng sau đây.\n",
    "Giấy phép lái xe hạng D được cấp cho người lái các loại xe ô tô quy định cho giấy phép lái xe hạng D.\n",
    "Giấy phép lái xe hạng D được cấp cho người lái rơ moóc có khối lượng toàn bộ theo thiết kế trên 750 kg.\n",
    "Giấy phép lái xe hạng D được cấp cho người lái xe ô tô chở khách nối toa.\n",
    "\"\"\""
   ],
   "id": "2b1ee666e32982b4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T07:26:56.587122Z",
     "start_time": "2025-10-11T07:26:56.577240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vncorenlp_pos_map = {\n",
    "    \"N\":   \"Noun (Danh từ)\",\n",
    "    \"Np\":  \"Proper noun (Danh từ riêng)\",\n",
    "    \"Nc\":  \"Classifier noun (Danh từ giống loại)\",\n",
    "    \"Nu\":  \"Unit noun (Danh từ đơn vị)\",\n",
    "    \"V\":   \"Verb (Động từ)\",\n",
    "    \"Vb\":  \"Verb (base) (Động từ gốc)\",\n",
    "    \"A\":   \"Adjective (Tính từ)\",\n",
    "    \"Ai\":  \"Adjective (predicative) (Tính từ vị ngữ)\",\n",
    "    \"P\":   \"Pronoun (Đại từ)\",\n",
    "    \"R\":   \"Adverb (Trạng từ)\",\n",
    "    \"M\":   \"Numeral / number (Số từ)\",\n",
    "    \"E\":   \"Preposition / particle (Giới từ / trợ từ)\",\n",
    "    \"C\":   \"Coordinating conjunction (Liên từ phối hợp)\",\n",
    "    \"CC\":  \"Subordinating conjunction / complementizer (Liên từ phụ thuộc)\",\n",
    "    \"L\":   \"Determiner / article (Từ hạn định)\",\n",
    "    \"D\":   \"Adverbial marker / degree marker (Từ chỉ mức độ)\",\n",
    "    \"X\":   \"Other (Khác)\",\n",
    "    \"CH\":  \"Punctuation (Dấu câu)\"\n",
    "}\n",
    "\n",
    "def process_sentence(text: str, rdrsegmenter, verbose: bool = True) -> dict:\n",
    "    results = {\n",
    "        \"original\": text,\n",
    "        \"cleaned\": \"\",\n",
    "        \"segmented\": [],\n",
    "        \"pos_annotation\": [],\n",
    "        \"concepts\": []\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"1. Original text:\\n\", text, \"\\n\")\n",
    "\n",
    "    # 2. Clean\n",
    "    text = clean_text(text)\n",
    "    results[\"cleaned\"] = text\n",
    "    if verbose:\n",
    "        print(\"2. Cleaned text:\\n\", text, \"\\n\")\n",
    "\n",
    "    # 3. Segmentation\n",
    "    segmented = rdrsegmenter.word_segment(text)\n",
    "    results[\"segmented\"] = segmented\n",
    "    if verbose:\n",
    "        print(\"3. Segmented text (tokens):\\n\", segmented, \"\\n\")\n",
    "\n",
    "    # 4. POS tagging\n",
    "    output = rdrsegmenter.annotate_text(text)\n",
    "    sents = output.values() if isinstance(output, dict) else output\n",
    "\n",
    "    pos_annot = []\n",
    "    tokens = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"4. POS & Head & DepRel annotation:\")\n",
    "        print(f\"{'Idx':<5} {'Token':<15} {'Head':<5} {'DepRel':<15} {'POS':<25}\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "    for sent in sents:\n",
    "        if not isinstance(sent, list):\n",
    "            continue\n",
    "        for token in sent:\n",
    "            if not isinstance(token, dict):\n",
    "                continue\n",
    "            word = token.get(\"wordForm\", \"\")\n",
    "            pos = token.get(\"posTag\", \"\")\n",
    "            head = token.get(\"head\", \"\")\n",
    "            dep = token.get(\"depLabel\", \"\")\n",
    "            pos_full = vncorenlp_pos_map.get(pos, pos)\n",
    "\n",
    "            pos_data = {\n",
    "                \"index\": token.get(\"index\", \"\"),\n",
    "                \"token\": word,\n",
    "                \"pos\": pos_full,\n",
    "                \"head\": head,\n",
    "                \"dep\": dep\n",
    "            }\n",
    "            pos_annot.append(pos_data)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{pos_data['index']:<5} {pos_data['token']:<15} {pos_data['head']:<5} {pos_data['dep']:<15} {pos_data['pos']:<25}\")\n",
    "\n",
    "            # Only append N, V, A, E for triplet extraction\n",
    "            if pos.startswith((\"N\", \"V\", \"A\", \"E\", \"M\")):\n",
    "                tokens.append((word.replace(\"_\", \" \"), pos))\n",
    "\n",
    "    results[\"pos_annotation\"] = pos_annot\n",
    "\n",
    "    # 5. Triplet extraction\n",
    "    concepts = []\n",
    "    concept1_tokens = []\n",
    "    concept2_tokens = []\n",
    "    relation_tokens = []\n",
    "\n",
    "    for token, pos in tokens:\n",
    "        if pos.startswith(\"V\"):\n",
    "            if concept2_tokens:\n",
    "                triplet = (\n",
    "                    \" \".join(concept1_tokens),\n",
    "                    \" \".join(relation_tokens),\n",
    "                    \" \".join(concept2_tokens)\n",
    "                )\n",
    "                if triplet not in concepts:  # chỉ thêm nếu chưa tồn tại\n",
    "                    concepts.append(triplet)\n",
    "\n",
    "                concept1_tokens = concept2_tokens\n",
    "                concept2_tokens = []\n",
    "                relation_tokens = []\n",
    "\n",
    "            relation_tokens.append(token)\n",
    "        else:\n",
    "            if relation_tokens:\n",
    "                concept2_tokens.append(token)\n",
    "            else:\n",
    "                concept1_tokens.append(token)\n",
    "\n",
    "    # Append last triplet if complete\n",
    "    if concept1_tokens and relation_tokens and concept2_tokens:\n",
    "        concepts.append((\n",
    "            \" \".join(concept1_tokens),\n",
    "            \" \".join(relation_tokens),\n",
    "            \" \".join(concept2_tokens)\n",
    "        ))\n",
    "\n",
    "    results[\"concepts\"] = concepts\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n5. Extracted triplets:\")\n",
    "        for c in concepts:\n",
    "            print(c)\n",
    "\n",
    "    return results"
   ],
   "id": "dd31fc67f9b553bb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:25:42.662688Z",
     "start_time": "2025-10-11T06:25:42.650091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# giữ map POS của bạn\n",
    "vncorenlp_pos_map = {\n",
    "    \"N\":   \"Noun (Danh từ)\",\n",
    "    \"Np\":  \"Proper noun (Danh từ riêng)\",\n",
    "    \"Nc\":  \"Classifier noun (Danh từ giống loại)\",\n",
    "    \"Nu\":  \"Unit noun (Danh từ đơn vị)\",\n",
    "    \"V\":   \"Verb (Động từ)\",\n",
    "    \"Vb\":  \"Verb (base) (Động từ gốc)\",\n",
    "    \"A\":   \"Adjective (Tính từ)\",\n",
    "    \"Ai\":  \"Adjective (predicative) (Tính từ vị ngữ)\",\n",
    "    \"P\":   \"Pronoun (Đại từ)\",\n",
    "    \"R\":   \"Adverb (Trạng từ)\",\n",
    "    \"M\":   \"Numeral / number (Số từ)\",\n",
    "    \"E\":   \"Preposition / particle (Giới từ / trợ từ)\",\n",
    "    \"C\":   \"Coordinating conjunction (Liên từ phối hợp)\",\n",
    "    \"CC\":  \"Subordinating conjunction / complementizer (Liên từ phụ thuộc)\",\n",
    "    \"L\":   \"Determiner / article (Từ hạn định)\",\n",
    "    \"D\":   \"Adverbial marker / degree marker (Từ chỉ mức độ)\",\n",
    "    \"X\":   \"Other (Khác)\",\n",
    "    \"CH\":  \"Punctuation (Dấu câu)\"\n",
    "}\n",
    "\n",
    "# ----- Các cấu hình / stoplists / helper sets -----\n",
    "AUX_WORDS = {\"được\", \"bị\", \"đang\", \"đã\", \"sẽ\"}        # trợ động từ hay gặp để gộp với động từ chính\n",
    "NO_OBJECT_TOKENS = {\"sau\", \"như\", \"như sau\", \"ví dụ\", \"v.v\", \"v.v.\"}\n",
    "DETERMINERS = {\"các\", \"những\", \"một\", \"mỗi\", \"toàn_bộ\", \"tất_cả\", \"cả\"}\n",
    "# Những danh từ như \"trách nhiệm\" mà thường theo sau bởi chuỗi động từ cần kéo vào object\n",
    "N_HEADS_WITH_VERB_MOD = {\"trách nhiệm\", \"nhiệm vụ\", \"quyền\", \"bổn phận\", \"nghĩa vụ\"}\n",
    "\n",
    "# Các helper\n",
    "def is_np_pos(pos):\n",
    "    return isinstance(pos, str) and pos.startswith(\"N\")\n",
    "\n",
    "def is_verb_pos(pos):\n",
    "    return isinstance(pos, str) and pos.startswith(\"V\")\n",
    "\n",
    "def is_prep_pos(pos):\n",
    "    return isinstance(pos, str) and pos.startswith(\"E\")\n",
    "\n",
    "def normalize_token(tok: str) -> str:\n",
    "    return tok.replace(\"_\", \" \").strip()\n",
    "\n",
    "# Gộp trợ động từ + động từ chính nếu có\n",
    "def merge_aux_verbs(tokens_pos):\n",
    "    \"\"\"\n",
    "    tokens_pos: list of (token_str, pos_tag_short)\n",
    "    Trả về list đã gộp, ví dụ ('được', 'V') + ('quy định','V') -> ('được quy định', 'V')\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(tokens_pos):\n",
    "        tok, pos = tokens_pos[i]\n",
    "        # nếu token là trợ động từ (được, bị, ...) và tiếp theo là verb => gộp\n",
    "        if tok in AUX_WORDS and i + 1 < len(tokens_pos):\n",
    "            nxt_tok, nxt_pos = tokens_pos[i+1]\n",
    "            if is_verb_pos(nxt_pos):\n",
    "                merged.append((f\"{tok} {nxt_tok}\", nxt_pos))\n",
    "                i += 2\n",
    "                continue\n",
    "        # ngược lại, giữ nguyên\n",
    "        merged.append((tok, pos))\n",
    "        i += 1\n",
    "    return merged\n",
    "\n",
    "# Mở rộng NP (backward/forward). Nếu head là 1 trong N_HEADS_WITH_VERB_MOD, kéo theo verb modifiers\n",
    "def expand_np_with_post_verbs(tokens_pos, start_idx, direction=\"forward\"):\n",
    "    \"\"\"\n",
    "    start_idx: index to begin expansion (for forward: start_idx is index of the first NP token; for backward: index of last NP token)\n",
    "    direction: \"forward\" or \"backward\"\n",
    "    \"\"\"\n",
    "    np_tokens = []\n",
    "    i = start_idx\n",
    "    step = 1 if direction == \"forward\" else -1\n",
    "    # collect N / E tokens (simple heuristic)\n",
    "    while 0 <= i < len(tokens_pos):\n",
    "        t, p = tokens_pos[i]\n",
    "        if is_np_pos(p) or is_prep_pos(p) or (p and p.startswith(\"L\")):  # include determiners marked L if present\n",
    "            if direction == \"backward\":\n",
    "                np_tokens.insert(0, t)\n",
    "            else:\n",
    "                np_tokens.append(t)\n",
    "            i += step\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # If forward expansion and last token is a head that should pull verb modifiers, then pull following verbs\n",
    "    if direction == \"forward\" and np_tokens:\n",
    "        last = normalize_token(np_tokens[-1])\n",
    "        if last in N_HEADS_WITH_VERB_MOD:\n",
    "            j = start_idx + len(np_tokens)\n",
    "            # append consecutive verbs immediately after\n",
    "            while j < len(tokens_pos):\n",
    "                t_j, p_j = tokens_pos[j]\n",
    "                if is_verb_pos(p_j):\n",
    "                    np_tokens.append(t_j)\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return np_tokens\n",
    "\n",
    "# Main extractor using POS heuristics + NP expansion + aux merge\n",
    "def extract_triplets_from_pos_annot(pos_annot):\n",
    "    \"\"\"\n",
    "    pos_annot: list of dicts with at least:\n",
    "        - \"index\"\n",
    "        - \"token\" (token string, underscores possible)\n",
    "        - \"pos\" or \"posTag\" (short tag like 'N', 'Vb', etc.)\n",
    "    Returns list of (subject, relation, object)\n",
    "    \"\"\"\n",
    "    # Normalize tokens_pos to list of (token_str, pos_short)\n",
    "    tokens_pos = []\n",
    "    for d in pos_annot:\n",
    "        t = normalize_token(d.get(\"token\", \"\"))\n",
    "        # try to obtain short pos tag\n",
    "        pos_short = d.get(\"posTag\") or d.get(\"pos\") or \"\"\n",
    "        if isinstance(pos_short, str) and \" \" in pos_short:\n",
    "            pos_short = pos_short.split()[0]\n",
    "        tokens_pos.append((t, pos_short))\n",
    "\n",
    "    # Merge auxiliaries\n",
    "    tokens_pos = merge_aux_verbs(tokens_pos)\n",
    "\n",
    "    triplets = []\n",
    "    i = 0\n",
    "    while i < len(tokens_pos):\n",
    "        tok, pos = tokens_pos[i]\n",
    "\n",
    "        # If current is a verb -> candidate relation\n",
    "        if is_verb_pos(pos):\n",
    "            relation = tok\n",
    "\n",
    "            # SUBJECT: look backward for nearest NP (take last NP before verb), include determiner if adjacent\n",
    "            subj = []\n",
    "            j = i - 1\n",
    "            while j >= 0:\n",
    "                t_j, p_j = tokens_pos[j]\n",
    "                if is_np_pos(p_j):\n",
    "                    # expand NP backward (find extent)\n",
    "                    # find the starting index of this NP (scan backward until not N/E)\n",
    "                    start_idx = j\n",
    "                    while start_idx - 1 >= 0 and (is_np_pos(tokens_pos[start_idx - 1][1]) or is_prep_pos(tokens_pos[start_idx - 1][1])):\n",
    "                        start_idx -= 1\n",
    "                    subj = expand_np_with_post_verbs(tokens_pos, start_idx, direction=\"forward\")\n",
    "                    # include determiner immediately before NP if present\n",
    "                    if start_idx - 1 >= 0:\n",
    "                        prev_tok, prev_pos = tokens_pos[start_idx - 1]\n",
    "                        if prev_tok in DETERMINERS:\n",
    "                            subj.insert(0, prev_tok)\n",
    "                    break\n",
    "                elif t_j in DETERMINERS:\n",
    "                    # keep searching — maybe determiner then noun before that\n",
    "                    j -= 1\n",
    "                    continue\n",
    "                else:\n",
    "                    j -= 1\n",
    "\n",
    "            # OBJECT: look forward for nearest NP\n",
    "            obj = []\n",
    "            k = i + 1\n",
    "            while k < len(tokens_pos):\n",
    "                t_k, p_k = tokens_pos[k]\n",
    "                if is_np_pos(p_k):\n",
    "                    # expand NP forward from k\n",
    "                    obj = expand_np_with_post_verbs(tokens_pos, k, direction=\"forward\")\n",
    "                    break\n",
    "                # special-case: relation == \"có\" -> expect \"có <N> <V*>\" pattern; treat next N as object\n",
    "                if relation == \"có\" and is_np_pos(p_k):\n",
    "                    obj = expand_np_with_post_verbs(tokens_pos, k, direction=\"forward\")\n",
    "                    break\n",
    "                # if sees punctuation or full stop before NP, stop\n",
    "                if p_k and p_k.startswith(\"CH\"):  # punctuation token class in your map\n",
    "                    break\n",
    "                # if next is another verb and no NP later, likely no object for this relation\n",
    "                if is_verb_pos(p_k):\n",
    "                    # allow sequences like \"phối_hợp giải_quyết\" only if they were attached to an earlier head (handled in NP expansion)\n",
    "                    break\n",
    "                k += 1\n",
    "\n",
    "            subj_str = \" \".join(subj).strip()\n",
    "            obj_str = \" \".join(obj).strip()\n",
    "\n",
    "            # Filter meaningless objects\n",
    "            if obj_str and obj_str not in NO_OBJECT_TOKENS:\n",
    "                if subj_str:\n",
    "                    trip = (subj_str, relation, obj_str)\n",
    "                    if trip not in triplets:\n",
    "                        triplets.append(trip)\n",
    "            else:\n",
    "                # Optionally store incomplete triplets (subject, relation, None) - commented out by default\n",
    "                # if subj_str:\n",
    "                #     triplets.append((subj_str, relation, None))\n",
    "                pass\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return triplets\n",
    "\n",
    "# ----------------- Phiên bản process_sentence đã cập nhật -----------------\n",
    "def process_sentence(text: str, rdrsegmenter, verbose: bool = True) -> dict:\n",
    "    results = {\n",
    "        \"original\": text,\n",
    "        \"cleaned\": \"\",\n",
    "        \"segmented\": [],\n",
    "        \"pos_annotation\": [],\n",
    "        \"concepts\": []\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"1. Original text:\\n\", text, \"\\n\")\n",
    "\n",
    "    # 2. Clean (giữ nguyên clean_text bạn đang dùng)\n",
    "    # giả định bạn có hàm clean_text — nếu không có hãy thay bằng: text = text.strip().lower()\n",
    "    try:\n",
    "        text = clean_text(text)\n",
    "    except NameError:\n",
    "        # fallback nhẹ nếu không có clean_text\n",
    "        text = text.strip().lower()\n",
    "    results[\"cleaned\"] = text\n",
    "    if verbose:\n",
    "        print(\"2. Cleaned text:\\n\", text, \"\\n\")\n",
    "\n",
    "    # 3. Segmentation\n",
    "    try:\n",
    "        segmented = rdrsegmenter.word_segment(text)\n",
    "    except Exception:\n",
    "        # fallback: treat whole text as one segment\n",
    "        segmented = [text]\n",
    "    results[\"segmented\"] = segmented\n",
    "    if verbose:\n",
    "        print(\"3. Segmented text (tokens):\\n\", segmented, \"\\n\")\n",
    "\n",
    "    # 4. POS tagging / annotation\n",
    "    output = rdrsegmenter.annotate_text(text)\n",
    "    # annotate_text có thể trả dict of sents hoặc list\n",
    "    sents = output.values() if isinstance(output, dict) else output\n",
    "\n",
    "    pos_annot = []\n",
    "    if verbose:\n",
    "        print(\"4. POS annotation:\")\n",
    "        print(f\"{'Idx':<5} {'Token':<20} {'POS':<30}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    for sent in sents:\n",
    "        if not isinstance(sent, list):\n",
    "            continue\n",
    "        for token in sent:\n",
    "            if not isinstance(token, dict):\n",
    "                continue\n",
    "            word = token.get(\"wordForm\", token.get(\"token\", \"\"))\n",
    "            pos_short = token.get(\"posTag\", \"\") or token.get(\"pos\", \"\")\n",
    "            pos_full = vncorenlp_pos_map.get(pos_short, pos_short)\n",
    "            idx = token.get(\"index\", \"\")\n",
    "\n",
    "            pos_data = {\n",
    "                \"index\": idx,\n",
    "                \"token\": word,\n",
    "                \"pos\": pos_full,       # full human-readable\n",
    "                \"posTag\": pos_short    # short tag for heuristics\n",
    "            }\n",
    "            pos_annot.append(pos_data)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{str(idx):<5} {normalize_token(word):<20} {pos_full:<30}\")\n",
    "\n",
    "    results[\"pos_annotation\"] = pos_annot\n",
    "\n",
    "    # 5. Triplet extraction (dùng hàm mới)\n",
    "    concepts = extract_triplets_from_pos_annot(pos_annot)\n",
    "    results[\"concepts\"] = concepts\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n5. Extracted triplets:\")\n",
    "        for c in concepts:\n",
    "            print(c)\n",
    "\n",
    "    return results\n"
   ],
   "id": "27551f3c03bc5dbf",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T05:16:41.646549Z",
     "start_time": "2025-10-11T05:16:41.643043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def insert_triplet(tx, concept1, relation, concept2, c1_metadata=None, c2_metadata=None, rel_metadata=None):\n",
    "    query = \"\"\"\n",
    "    MERGE (c1:Concept {name: $concept1})\n",
    "    ON CREATE SET c1 = $c1_metadata\n",
    "    ON MATCH SET\n",
    "        c1.document_number = apoc.coll.toSet(coalesce(c1.document_number, []) + [$c1_metadata.document_number]),\n",
    "        c1.title           = apoc.coll.toSet(coalesce(c1.title, []) + [$c1_metadata.title]),\n",
    "        c1.document_id     = apoc.coll.toSet(coalesce(c1.document_id, []) + [$c1_metadata.document_id])\n",
    "\n",
    "    MERGE (c2:Concept {name: $concept2})\n",
    "    ON CREATE SET c2 = $c2_metadata\n",
    "    ON MATCH SET\n",
    "        c2.document_number = apoc.coll.toSet(coalesce(c2.document_number, []) + [$c2_metadata.document_number]),\n",
    "        c2.title           = apoc.coll.toSet(coalesce(c2.title, []) + [$c2_metadata.title]),\n",
    "        c2.document_id     = apoc.coll.toSet(coalesce(c2.document_id, []) + [$c2_metadata.document_id])\n",
    "\n",
    "    MERGE (c1)-[r:RELATION {name: $relation}]->(c2)\n",
    "    ON CREATE SET r = $rel_metadata\n",
    "    ON MATCH SET  r += $rel_metadata\n",
    "\n",
    "    RETURN c1, r, c2\n",
    "    \"\"\"\n",
    "    tx.run(\n",
    "        query,\n",
    "        concept1=concept1,\n",
    "        concept2=concept2,\n",
    "        relation=relation,\n",
    "        c1_metadata=c1_metadata or {},\n",
    "        c2_metadata=c2_metadata or {},\n",
    "        rel_metadata=rel_metadata or {}\n",
    "    )\n",
    "\n",
    "def delete_all(tx):\n",
    "    # Delete all nodes and relationships\n",
    "    tx.run(\"MATCH (n) DETACH DELETE n\")"
   ],
   "id": "11908dd4e431625c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T05:16:42.623951Z",
     "start_time": "2025-10-11T05:16:42.620309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_triplet_and_store(input_text: str, rdrsegmenter, driver, db_name, document_id, document_number):\n",
    "    with driver.session(database=db_name) as session:\n",
    "        split_text = input_text.split(\"\\n\")\n",
    "        for t in split_text:\n",
    "            res = process_sentence(t, rdrsegmenter, verbose=False)\n",
    "            for concept1, relation, concept2 in res[\"concepts\"]:\n",
    "                session.execute_write(\n",
    "                    insert_triplet,\n",
    "                    concept1,\n",
    "                    relation,\n",
    "                    concept2,\n",
    "                    c1_metadata={\"document_number\": document_number, \"document_id\": document_id},\n",
    "                    c2_metadata={\"document_number\": document_number, \"document_id\": document_id},\n",
    "                    rel_metadata={\"document_number\": document_number, \"document_id\": document_id},\n",
    "                )"
   ],
   "id": "1c1a8ba1dd916b16",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:46:47.391993Z",
     "start_time": "2025-10-11T06:46:47.388522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_triplet(input_text: str, rdrsegmenter, verbose=False):\n",
    "    result = []\n",
    "    split_text = input_text.split(\"\\n\")\n",
    "    for t in split_text:\n",
    "        if not t:\n",
    "            continue\n",
    "        res = process_sentence(t, rdrsegmenter, verbose=verbose)\n",
    "        result.append(res[\"concepts\"])\n",
    "    return result"
   ],
   "id": "f609d92ba594712e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T16:25:11.243946Z",
     "start_time": "2025-10-13T16:24:59.325482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"rdrsegmenter\" not in globals():\n",
    "    import py_vncorenlp\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(\n",
    "        annotators=[\"wseg\", \"pos\",\"ner\",\"parse\"],\n",
    "        save_dir=r\"E:\\Github\\uit_chatbot\\graph\\VnCoreNLP-1.2\"\n",
    "    )"
   ],
   "id": "caf5ec05f64536c2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T05:22:23.316659Z",
     "start_time": "2025-10-11T05:19:19.076086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dep_labels = set()\n",
    "for i in range(10000):\n",
    "    rows = extract_random_from_db(cursor, True)\n",
    "    law = \"\"\n",
    "    title = \"\"\n",
    "    for r in rows:\n",
    "        title += r['title'] + \" \"\n",
    "        if not (r['title'].strip().startswith(\"Chương\") or r['title'].strip().startswith(\"Điều\")):\n",
    "            if r['content']:\n",
    "                law += r['content'] + \"\\n\"\n",
    "\n",
    "    text = clean_text(law)\n",
    "    output = rdrsegmenter.annotate_text(text)\n",
    "\n",
    "    for sentence in output.values():\n",
    "        for token in sentence:\n",
    "            dep_labels.add(token[\"depLabel\"])\n",
    "\n",
    "# in ra các nhãn duy nhất\n",
    "for label in dep_labels:\n",
    "    print(label)"
   ],
   "id": "809c9eaadeeeac8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnr\n",
      "iob\n",
      "punct\n",
      "loc\n",
      "root\n",
      "cnc\n",
      "vmod\n",
      "coord\n",
      "xmdp\n",
      "dir\n",
      "dep\n",
      "pob\n",
      "conj\n",
      "cnd\n",
      "xtmp\n",
      "adv\n",
      "ext\n",
      "prp\n",
      "pmod\n",
      "xadv\n",
      "dob\n",
      "amod\n",
      "det\n",
      "sub\n",
      "xloc\n",
      "tmp\n",
      "x\n",
      "nmod\n",
      "prd\n",
      "tpc\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T16:49:47.639112Z",
     "start_time": "2025-10-13T16:49:47.474259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j://127.0.0.1:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"1234567890\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ],
   "id": "8a847a0a87f74288",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T06:48:14.705202Z",
     "start_time": "2025-10-11T06:48:14.659452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = extract_triplet(rewrite_sentence, rdrsegmenter, True)\n",
    "for r in result:\n",
    "    print(r)"
   ],
   "id": "b9ce1aa02f486a99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original text:\n",
      " Giấy phép lái xe bao gồm các hạng sau đây. \n",
      "\n",
      "2. Cleaned text:\n",
      " giấy phép lái xe bao gồm các hạng sau đây. \n",
      "\n",
      "3. Segmented text (tokens):\n",
      " ['giấy_phép lái_xe bao_gồm các hạng sau đây .'] \n",
      "\n",
      "4. POS & Head & DepRel annotation:\n",
      "Idx   Token           Head  DepRel          POS                      \n",
      "---------------------------------------------------------------------------\n",
      "1     giấy_phép       3     sub             Noun (Danh từ)           \n",
      "2     lái_xe          1     nmod            Verb (Động từ)           \n",
      "3     bao_gồm         0     root            Verb (Động từ)           \n",
      "4     các             5     det             Determiner / article (Từ hạn định)\n",
      "5     hạng            3     dob             Noun (Danh từ)           \n",
      "6     sau             5     nmod            Noun (Danh từ)           \n",
      "7     đây             5     det             Pronoun (Đại từ)         \n",
      "8     .               3     punct           Punctuation (Dấu câu)    \n",
      "\n",
      "5. Extracted triplets:\n",
      "('giấy phép', 'lái xe bao gồm', 'hạng sau')\n",
      "1. Original text:\n",
      " Giấy phép lái xe hạng D được cấp cho người lái các loại xe ô tô quy định cho giấy phép lái xe hạng D. \n",
      "\n",
      "2. Cleaned text:\n",
      " giấy phép lái xe hạng d được cấp cho người lái các loại xe ô tô quy định cho giấy phép lái xe hạng d. \n",
      "\n",
      "3. Segmented text (tokens):\n",
      " ['giấy_phép lái_xe hạng d được cấp cho người lái các loại xe ô_tô quy_định cho giấy_phép lái_xe hạng d .'] \n",
      "\n",
      "4. POS & Head & DepRel annotation:\n",
      "Idx   Token           Head  DepRel          POS                      \n",
      "---------------------------------------------------------------------------\n",
      "1     giấy_phép       5     sub             Noun (Danh từ)           \n",
      "2     lái_xe          1     nmod            Verb (Động từ)           \n",
      "3     hạng            1     nmod            Noun (Danh từ)           \n",
      "4     d               3     nmod            Ny                       \n",
      "5     được            0     root            Verb (Động từ)           \n",
      "6     cấp             5     vmod            Verb (Động từ)           \n",
      "7     cho             6     iob             Preposition / particle (Giới từ / trợ từ)\n",
      "8     người           7     pob             Noun (Danh từ)           \n",
      "9     lái             8     nmod            Verb (Động từ)           \n",
      "10    các             11    det             Determiner / article (Từ hạn định)\n",
      "11    loại            6     dob             Classifier noun (Danh từ giống loại)\n",
      "12    xe              11    nmod            Noun (Danh từ)           \n",
      "13    ô_tô            11    nmod            Noun (Danh từ)           \n",
      "14    quy_định        11    nmod            Verb (Động từ)           \n",
      "15    cho             14    vmod            Preposition / particle (Giới từ / trợ từ)\n",
      "16    giấy_phép       15    pob             Noun (Danh từ)           \n",
      "17    lái_xe          16    nmod            Noun (Danh từ)           \n",
      "18    hạng            16    nmod            Noun (Danh từ)           \n",
      "19    d               18    nmod            Ny                       \n",
      "20    .               5     punct           Punctuation (Dấu câu)    \n",
      "\n",
      "5. Extracted triplets:\n",
      "('giấy phép', 'lái xe', 'hạng d')\n",
      "('hạng d', 'được cấp', 'cho người')\n",
      "('cho người', 'lái', 'loại xe ô tô')\n",
      "('loại xe ô tô', 'quy định', 'cho giấy phép lái xe hạng d')\n",
      "1. Original text:\n",
      " Giấy phép lái xe hạng D được cấp cho người lái rơ moóc có khối lượng toàn bộ theo thiết kế trên 750 kg. \n",
      "\n",
      "2. Cleaned text:\n",
      " giấy phép lái xe hạng d được cấp cho người lái rơ moóc có khối lượng toàn bộ theo thiết kế trên 750 kg. \n",
      "\n",
      "3. Segmented text (tokens):\n",
      " ['giấy_phép lái_xe hạng d được cấp cho người lái rơ moóc có khối_lượng toàn_bộ theo thiết_kế trên 750 kg .'] \n",
      "\n",
      "4. POS & Head & DepRel annotation:\n",
      "Idx   Token           Head  DepRel          POS                      \n",
      "---------------------------------------------------------------------------\n",
      "1     giấy_phép       5     sub             Noun (Danh từ)           \n",
      "2     lái_xe          1     nmod            Verb (Động từ)           \n",
      "3     hạng            1     nmod            Noun (Danh từ)           \n",
      "4     d               3     nmod            Ny                       \n",
      "5     được            0     root            Verb (Động từ)           \n",
      "6     cấp             5     vmod            Verb (Động từ)           \n",
      "7     cho             6     iob             Preposition / particle (Giới từ / trợ từ)\n",
      "8     người           7     pob             Noun (Danh từ)           \n",
      "9     lái             8     nmod            Verb (Động từ)           \n",
      "10    rơ              9     vmod            Verb (Động từ)           \n",
      "11    moóc            8     nmod            Noun (Danh từ)           \n",
      "12    có              8     nmod            Verb (Động từ)           \n",
      "13    khối_lượng      12    dob             Noun (Danh từ)           \n",
      "14    toàn_bộ         19    det             Determiner / article (Từ hạn định)\n",
      "15    theo            13    nmod            Verb (Động từ)           \n",
      "16    thiết_kế        15    vmod            Verb (Động từ)           \n",
      "17    trên            12    loc             Preposition / particle (Giới từ / trợ từ)\n",
      "18    750             19    det             Numeral / number (Số từ) \n",
      "19    kg              17    pob             Unit noun (Danh từ đơn vị)\n",
      "20    .               5     punct           Punctuation (Dấu câu)    \n",
      "\n",
      "5. Extracted triplets:\n",
      "('giấy phép', 'lái xe', 'hạng d')\n",
      "('hạng d', 'được cấp', 'cho người')\n",
      "('cho người', 'lái rơ', 'moóc')\n",
      "('moóc', 'có', 'khối lượng')\n",
      "('khối lượng', 'theo thiết kế', 'trên kg')\n",
      "1. Original text:\n",
      " Giấy phép lái xe hạng D được cấp cho người lái xe ô tô chở khách nối toa. \n",
      "\n",
      "2. Cleaned text:\n",
      " giấy phép lái xe hạng d được cấp cho người lái xe ô tô chở khách nối toa. \n",
      "\n",
      "3. Segmented text (tokens):\n",
      " ['giấy_phép lái_xe hạng d được cấp cho người lái_xe ô_tô chở khách nối toa .'] \n",
      "\n",
      "4. POS & Head & DepRel annotation:\n",
      "Idx   Token           Head  DepRel          POS                      \n",
      "---------------------------------------------------------------------------\n",
      "1     giấy_phép       5     sub             Noun (Danh từ)           \n",
      "2     lái_xe          1     nmod            Verb (Động từ)           \n",
      "3     hạng            1     nmod            Noun (Danh từ)           \n",
      "4     d               3     nmod            Ny                       \n",
      "5     được            0     root            Verb (Động từ)           \n",
      "6     cấp             5     vmod            Verb (Động từ)           \n",
      "7     cho             6     iob             Preposition / particle (Giới từ / trợ từ)\n",
      "8     người           7     pob             Noun (Danh từ)           \n",
      "9     lái_xe          8     nmod            Verb (Động từ)           \n",
      "10    ô_tô            9     dob             Noun (Danh từ)           \n",
      "11    chở             8     nmod            Verb (Động từ)           \n",
      "12    khách           11    dob             Noun (Danh từ)           \n",
      "13    nối             8     nmod            Verb (Động từ)           \n",
      "14    toa             13    dob             Noun (Danh từ)           \n",
      "15    .               5     punct           Punctuation (Dấu câu)    \n",
      "\n",
      "5. Extracted triplets:\n",
      "('giấy phép', 'lái xe', 'hạng d')\n",
      "('hạng d', 'được cấp', 'cho người')\n",
      "('cho người', 'lái xe', 'ô tô')\n",
      "('ô tô', 'chở', 'khách')\n",
      "('khách', 'nối', 'toa')\n",
      "[('giấy phép', 'lái xe bao gồm', 'hạng sau')]\n",
      "[('giấy phép', 'lái xe', 'hạng d'), ('hạng d', 'được cấp', 'cho người'), ('cho người', 'lái', 'loại xe ô tô'), ('loại xe ô tô', 'quy định', 'cho giấy phép lái xe hạng d')]\n",
      "[('giấy phép', 'lái xe', 'hạng d'), ('hạng d', 'được cấp', 'cho người'), ('cho người', 'lái rơ', 'moóc'), ('moóc', 'có', 'khối lượng'), ('khối lượng', 'theo thiết kế', 'trên kg')]\n",
      "[('giấy phép', 'lái xe', 'hạng d'), ('hạng d', 'được cấp', 'cho người'), ('cho người', 'lái xe', 'ô tô'), ('ô tô', 'chở', 'khách'), ('khách', 'nối', 'toa')]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_triplet_and_store(rewrite_sentence, rdrsegmenter, driver, \"ontology\", id, so_hieu)",
   "id": "651c6adfb9d10721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T08:04:44.833615Z",
     "start_time": "2025-10-03T08:04:44.818386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with driver.session(database=\"ontology\") as session:\n",
    "    session.execute_write(delete_all)"
   ],
   "id": "51176644fe62a605",
   "outputs": [],
   "execution_count": 139
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
